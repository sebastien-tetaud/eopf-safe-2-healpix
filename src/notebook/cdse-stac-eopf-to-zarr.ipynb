{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Sentinel-2 Data from the Copernicus Data Space Ecosystem\n",
    "\n",
    "This notebook demonstrates how to access and process Sentinel-2 satellite imagery from the Copernicus Data Space Ecosystem using both S3 and STAC interfaces.\n",
    "\n",
    "### What you'll learn:\n",
    "- How to connect to Copernicus Data Space Ecosystem using S3 credentials\n",
    "- How to search for satellite imagery using STAC API\n",
    "- Download CDSE .SAFE product\n",
    "- Convert SAFE file into a Zarr product\n",
    "- Download and transform on the fly .SAFE product into Zarr product\n",
    "- How to download and visualize Sentinel-2 imagery\n",
    "\n",
    "### Prerequisites:\n",
    "- Copernicus Data Space Ecosystem account (https://dataspace.copernicus.eu/)\n",
    "- Access and secret keys configured in environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import random\n",
    "\n",
    "import boto3\n",
    "import pystac_client\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Environment Configuration\n",
    "\n",
    "First, we'll import the required libraries and set up our environment. Make sure your Copernicus credentials are stored in your environment variables or a `.env` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get credentials from environment variables\n",
    "load_dotenv()\n",
    "ACCESS_KEY_ID = os.environ.get(\"ACCESS_KEY_ID\")\n",
    "SECRET_ACCESS_KEY = os.environ.get(\"SECRET_ACCESS_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The S3Connector Class\n",
    "\n",
    "The `S3Connector` class provides an interface to connect to the S3-compatible storage service of the Copernicus Data Space Ecosystem. This class handles authentication and connection management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3Connector:\n",
    "    \"\"\"A clean connector for S3-compatible storage services\"\"\"\n",
    "\n",
    "    def __init__(self, endpoint_url, access_key_id,\n",
    "                 secret_access_key, region_name='default'):\n",
    "        \"\"\"Initialize the S3Connector with connection parameters\"\"\"\n",
    "        self.endpoint_url = endpoint_url\n",
    "        self.access_key_id = access_key_id\n",
    "        self.secret_access_key = secret_access_key\n",
    "        self.region_name = region_name\n",
    "\n",
    "        # Create session\n",
    "        self.session = boto3.session.Session()\n",
    "\n",
    "        # Initialize S3 resource\n",
    "        self.s3 = self.session.resource(\n",
    "            's3',\n",
    "            endpoint_url=self.endpoint_url,\n",
    "            aws_access_key_id=self.access_key_id,\n",
    "            aws_secret_access_key=self.secret_access_key,\n",
    "            region_name=self.region_name\n",
    "        )\n",
    "\n",
    "        # Initialize S3 client\n",
    "        self.s3_client = self.session.client(\n",
    "            's3',\n",
    "            endpoint_url=self.endpoint_url,\n",
    "            aws_access_key_id=self.access_key_id,\n",
    "            aws_secret_access_key=self.secret_access_key,\n",
    "            region_name=self.region_name\n",
    "        )\n",
    "\n",
    "    def get_s3_client(self):\n",
    "        \"\"\"Get the boto3 S3 client\"\"\"\n",
    "        return self.s3_client\n",
    "\n",
    "    def get_s3_resource(self):\n",
    "        \"\"\"Get the boto3 S3 resource\"\"\"\n",
    "        return self.s3\n",
    "\n",
    "    def get_bucket(self, bucket_name):\n",
    "        \"\"\"Get a specific bucket by name\"\"\"\n",
    "        return self.s3.Bucket(bucket_name)\n",
    "\n",
    "    def list_buckets(self):\n",
    "        \"\"\"List all available buckets\"\"\"\n",
    "        response = self.s3_client.list_buckets()\n",
    "        if 'Buckets' in response:\n",
    "            return [bucket['Name'] for bucket in response['Buckets']]\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "The following function helps convert S3 URIs from the STAC catalog into S3 keys that can be used for direct access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s3_path_from_url(url):\n",
    "    \"\"\"\n",
    "    Extracts the S3 object path from an S3 URL or URI.\n",
    "\n",
    "    This function parses S3 URLs/URIs and returns just the object path portion,\n",
    "    removing the protocol (s3://), bucket name, and any leading slashes.\n",
    "\n",
    "    Args:\n",
    "        url (str): The full S3 URI (e.g., 's3://eodata/path/to/file.jp2')\n",
    "\n",
    "    Returns:\n",
    "        str: The S3 object path (without protocol, bucket name and leading slashes)\n",
    "    \"\"\"\n",
    "    # If it's not an S3 URI, return it unchanged\n",
    "    if not url.startswith('s3://'):\n",
    "        return url\n",
    "\n",
    "    # Parse the S3 URI\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # Ensure this is an S3 URL\n",
    "    if parsed_url.scheme != 's3':\n",
    "        raise ValueError(f\"URL {url} is not an S3 URL\")\n",
    "\n",
    "    # Extract the path without leading slashes\n",
    "    object_path = parsed_url.path.lstrip('/')\n",
    "\n",
    "    return object_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product(s3_resource, bucket_name, object_url, output_path):\n",
    "    \"\"\"\n",
    "    Download a product from S3 bucket and create output directory if it doesn't exist.\n",
    "\n",
    "    Args:\n",
    "        s3_resource: boto3 S3 resource object\n",
    "        bucket_name (str): Name of the S3 bucket\n",
    "        object_url (str): Path to the object within the bucket\n",
    "        output_path (str): Local directory to save the file\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # Extract filename from the object URL\n",
    "    _, filename = os.path.split(object_url)\n",
    "\n",
    "    # Full path where the file will be saved\n",
    "    local_file_path = os.path.join(output_path, filename)\n",
    "\n",
    "    print(f\"Downloading {object_url} to {local_file_path}...\")\n",
    "\n",
    "    try:\n",
    "        # Download the file from S3\n",
    "        s3_resource.Bucket(bucket_name).download_file(object_url, local_file_path)\n",
    "        print(f\"Successfully downloaded to {local_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return local_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Copernicus Data Space Ecosystem\n",
    "\n",
    "Now let's establish connections to both the S3 storage and STAC catalog services using our credentials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY_ID = os.environ.get(\"ACCESS_KEY_ID\")\n",
    "SECRET_ACCESS_KEY = os.environ.get(\"SECRET_ACCESS_KEY\")\n",
    "ENDPOINT_URL = 'https://eodata.dataspace.copernicus.eu'\n",
    "ENDPOINT_STAC = \"https://stac.dataspace.copernicus.eu/v1/\"\n",
    "BUCKET_NAME = \"eodata\"\n",
    "catalog = pystac_client.Client.open(ENDPOINT_STAC)\n",
    "connector = S3Connector(\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    access_key_id=ACCESS_KEY_ID,\n",
    "    secret_access_key=SECRET_ACCESS_KEY,\n",
    "    region_name='default'\n",
    ")\n",
    "# Get S3 client and resource from the connector instance\n",
    "s3 = connector.get_s3_resource()\n",
    "s3_client = connector.get_s3_client()\n",
    "buckets = connector.list_buckets()\n",
    "print(\"Available buckets:\", buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Sentinel-2 Product\n",
    "\n",
    "We'll use the STAC API to search for Sentinel-2 Level 2A products based on:\n",
    "- Geographic location (longitude/latitude point)\n",
    "- Date range\n",
    "- Cloud cover threshold\n",
    "\n",
    "The search results provide metadata and access links to the actual imagery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LON, LAT = 150.97, -20.92\n",
    "# Search for Sentinel-2 products\n",
    "items_txt = catalog.search(\n",
    "    collections=['sentinel-2-l1c'],\n",
    "    intersects=dict(type=\"Point\", coordinates=[LON, LAT]),\n",
    "    datetime=\"2024-05-01/2024-06-01\",\n",
    "    query=[\"eo:cloud_cover<50\"]\n",
    ").item_collection()\n",
    "selected_item = random.choice(items_txt)\n",
    "selected_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundinx Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounding box coordinates [min_lon, min_lat, max_lon, max_lat]\n",
    "min_lon, min_lat, max_lon, max_lat = 150.95,-20.92, 150.97, -20.95  # Example: 1° box around LON=15, LAT=50\n",
    "\n",
    "# Create a bounding box polygon (must be closed, so repeat the first point at the end)\n",
    "bbox_polygon = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [[\n",
    "        [min_lon, min_lat],  # Southwest corner\n",
    "        [max_lon, min_lat],  # Southeast corner\n",
    "        [max_lon, max_lat],  # Northeast corner\n",
    "        [min_lon, max_lat],  # Northwest corner\n",
    "        [min_lon, min_lat]   # Close the polygon by repeating the first point\n",
    "    ]]\n",
    "}\n",
    "\n",
    "# Search for Sentinel-2 products within the bounding box\n",
    "items_txt = catalog.search(\n",
    "    collections=['sentinel-2-l2a'],\n",
    "    intersects=bbox_polygon,\n",
    "    datetime=\"2024-05-01/2024-06-01\",\n",
    "    query=[\"eo:cloud_cover<10\"]\n",
    ").item_collection()\n",
    "selected_item = random.choice(items_txt)\n",
    "selected_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Sentinel-2 Product\n",
    "\n",
    "Once we've identified the product we want, we can download it using our S3 connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(bucket, product: str, target: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Downloads every file in the S3 bucket with the provided product prefix.\n",
    "    Creates a local folder named after the .SAFE directory (without the .SAFE extension).\n",
    "\n",
    "    Args:\n",
    "        bucket: boto3 Resource bucket object\n",
    "        product: Path to the product (e.g., 'Sentinel-2/MSI/L2A/.../S2B_MSIL2A_..._T56KKB_20240516T015827.SAFE/')\n",
    "        target: Local directory to save the files. Defaults to current directory.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded .SAFE directory (without the .SAFE extension)\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the product was not found in the bucket\n",
    "    \"\"\"\n",
    "    # Ensure the product path ends with '/'\n",
    "    if not product.endswith('/'):\n",
    "        product += '/'\n",
    "\n",
    "    # List files in the S3 prefix\n",
    "    files = list(bucket.objects.filter(Prefix=product))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Could not find any files for {product}\")\n",
    "\n",
    "    # Extract the .SAFE directory name (e.g., \"S2B_MSIL2A_20240516T001109_N0510_R073_T56KKB_20240516T015827.SAFE\")\n",
    "    safe_dir = os.path.basename(product.rstrip('/'))\n",
    "    if not safe_dir.endswith('.SAFE'):\n",
    "        raise ValueError(f\"Expected a .SAFE directory, got: {safe_dir}\")\n",
    "\n",
    "    # Create the local target directory (without the .SAFE extension)\n",
    "    # local_dir = safe_dir[:-5]  # Remove '.SAFE' from the name\n",
    "    local_path = os.path.join(target, safe_dir)\n",
    "\n",
    "    # Create the local directory structure\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "    # Download each file while preserving the relative structure\n",
    "    for file in files:\n",
    "        # Skip directory markers (S3 pseudo-folders)\n",
    "        if file.key.endswith('/'):\n",
    "            continue\n",
    "\n",
    "        # Compute the relative path inside the .SAFE directory\n",
    "        relative_path = os.path.relpath(file.key, product)\n",
    "        local_file_path = os.path.join(local_path, relative_path)\n",
    "\n",
    "        # Create parent directories if they don't exist\n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "        # Download the file\n",
    "        bucket.download_file(file.key, local_file_path)\n",
    "\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_item = random.choice(items_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "product_url , _ = os.path.split(selected_item.assets['safe_manifest'].href)\n",
    "product_url = extract_s3_path_from_url(product_url)\n",
    "safe_filename = download(bucket, product_url, target=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Replace .SAFE with .zarr\n",
    "zarr_filename =  os.path.split(safe_filename)[1].replace('.SAFE', '.zarr')\n",
    "# Join the current directory path with the new filename\n",
    "zarr_path = os.path.join(current_dir, zarr_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOPF converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting a local legacy product (safe format) into new Zarr format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eopf.store.convert import convert\n",
    "\n",
    "convert(safe_filename, zarr_path, mask_and_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_filename = download(bucket, product_url, target=\".\")\n",
    "convert(safe_filename, zarr_path, mask_and_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "dt = xr.open_datatree(zarr_path, engine=\"zarr\", mask_and_scale=False, chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = dt.measurements.reflectance.r10m\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "rgb = np.dstack([\n",
    "    ds['b04'].values,\n",
    "    ds['b03'].values,\n",
    "    ds['b02'].values\n",
    "])\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(rgb)\n",
    "plt.axis('off')\n",
    "plt.title('Sentinel-2 RGB Composite')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and transform on the flight from S3 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and convert a legacy product (safe format) into new Zarr format locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url , _ = os.path.split(selected_item.assets['safe_manifest'].href)\n",
    "# Replace .SAFE with .zarr\n",
    "zarr_filename =  os.path.split(product_url)[1].replace('.SAFE', '.zarr')\n",
    "# Join the current directory path with the new filename\n",
    "zarr_path = os.path.join(current_dir, zarr_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eopf.common.constants import OpeningMode\n",
    "from eopf.common.file_utils import AnyPath\n",
    "\n",
    "S3_CONFIG = {\n",
    "    \"key\": ACCESS_KEY_ID, # EDIT WITH YOUR S3 KEY\n",
    "    \"secret\": SECRET_ACCESS_KEY, # EDIT WITH YOUR S3 SECRET KEY\n",
    "    \"client_kwargs\": { \"endpoint_url\": ENDPOINT_URL, \"region_name\": \"default\"} # EDIT WITH YOUR CLIENT_KWARGS\n",
    "}\n",
    "# Add this parameter if you want to overwrite the output of the conversion if it already exists\n",
    "target_store_config = dict(mode=OpeningMode.CREATE_OVERWRITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the zarr store we don't need to add the storage_options argument because we want to write the target product locally\n",
    "convert(AnyPath(product_url, **S3_CONFIG), zarr_path, target_store_kwargs=target_store_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import s3fs\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "#from ipyleaflet import Map, Polygon\n",
    "#from shapely import geometry\n",
    "\n",
    "from distributed import LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import h3\n",
    "from pyproj import Transformer\n",
    "zarr_path = \"/home/ubuntu/project/eopf-safe-2-healpix/src/notebook/S2A_MSIL2A_20240511T001111_N0510_R073_T56KKB_20240511T025552.zarr\"\n",
    "dt = xr.open_datatree(zarr_path, engine=\"zarr\", mask_and_scale=False, chunks={})\n",
    "# 1. Load Zarr Data (already open via datacube presumably)\n",
    "ds = dt.measurements.reflectance.r60m\n",
    "b02 = ds['b02']\n",
    "\n",
    "# 3. Generate mesh of x/y coordinates\n",
    "x = b02['x'].values\n",
    "y = b02['y'].values\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "# 4. Convert UTM to lat/lon\n",
    "utm_crs = \"EPSG:32756\"  # Update to your actual UTM zone\n",
    "transformer = Transformer.from_crs(utm_crs, \"EPSG:4326\", always_xy=True)\n",
    "lon, lat = transformer.transform(xx, yy)\n",
    "# 5. Flatten arrays\n",
    "vals = b02.values.ravel()\n",
    "lat = lat.ravel()\n",
    "lon = lon.ravel()\n",
    "\n",
    "# 7. Map to H3\n",
    "resolution = 10\n",
    "h3_cells = [h3.latlng_to_cell(lat[i], lon[i], resolution) for i in tqdm(range(len(lat)))]\n",
    "h3_ids = [h3.str_to_int(c) for c in h3_cells]  # convert to uint64\n",
    "\n",
    "# 8. Aggregate to H3 cell average\n",
    "df = pd.DataFrame({'cell_id': h3_ids, 'value': vals})\n",
    "agg = df.groupby('cell_id').mean().reset_index()\n",
    "\n",
    "h3_dataset = xr.Dataset(\n",
    "    data_vars={\n",
    "        \"b02\": xr.DataArray(\n",
    "            data=agg[\"value\"].values.astype(np.float64),\n",
    "            dims=[\"cells\"],\n",
    "            coords={\"cell_ids\": (\"cells\", agg[\"cell_id\"].values)},\n",
    "            attrs={\n",
    "                \"grid_name\": \"h3\",\n",
    "                \"level\": resolution,\n",
    "                \"units\": \"reflectance\",\n",
    "                \"long_name\": \"BOA Reflectance Band 02\"\n",
    "            }\n",
    "        )\n",
    "    },\n",
    "    attrs={\n",
    "        \"Conventions\": \"COARDS\",\n",
    "        \"title\": \"Sentinel-2 Band 02 Reflectance (H3 Aggregated)\",\n",
    "        \"description\": (\n",
    "            \"BOA reflectance from MSI acquisition at spectral band 02 (490 nm), \"\n",
    "            f\"aggregated into H3 hexagonal cells at resolution {resolution}.\"\n",
    "        ),\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "h3_dataset.coords[\"cell_ids\"].attrs[\"grid_name\"] = \"h3\"\n",
    "h3_dataset.coords[\"cell_ids\"].attrs[\"level\"] = resolution\n",
    "h3_dataset.to_zarr(\"sentinel_b02_h3_dataset.zarr\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import xdggs\n",
    "\n",
    "import lonboard\n",
    "\n",
    "# Load your dataset (if not already in memory)\n",
    "original_ds = xr.open_zarr(\"sentinel_b02_h3_dataset.zarr\")\n",
    "ds = original_ds.pipe(xdggs.decode)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_centers = ds.dggs.cell_centers()\n",
    "cell_boundaries = ds.dggs.cell_boundaries()\n",
    "cell_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"b02\"].compute().dggs.explore(cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arrow_table(polygons, arr, coords=None):\n",
    "    from arro3.core import Array, ChunkedArray, Schema, Table\n",
    "\n",
    "    if coords is None:\n",
    "        coords = [\"latitude\", \"longitude\"]\n",
    "\n",
    "    array = Array.from_arrow(polygons)\n",
    "    name = arr.name or \"data\"\n",
    "    arrow_arrays = {\n",
    "        \"geometry\": array,\n",
    "        \"cell_ids\": ChunkedArray([Array.from_numpy(arr.coords[\"cell_ids\"])]),\n",
    "        name: ChunkedArray([Array.from_numpy(arr.data)]),\n",
    "    } | {\n",
    "        coord: ChunkedArray([Array.from_numpy(arr.coords[coord].data)])\n",
    "        for coord in coords\n",
    "        if coord in arr.coords\n",
    "    }\n",
    "\n",
    "    fields = [array.field.with_name(name) for name, array in arrow_arrays.items()]\n",
    "    schema = Schema(fields)\n",
    "\n",
    "    return Table.from_arrays(list(arrow_arrays.values()), schema=schema)\n",
    "\n",
    "\n",
    "def normalize(var, center=None):\n",
    "    from matplotlib.colors import CenteredNorm, Normalize\n",
    "\n",
    "    if center is None:\n",
    "        vmin = var.min(skipna=True)\n",
    "        vmax = var.max(skipna=True)\n",
    "        normalizer = Normalize(vmin=vmin, vmax=vmax)\n",
    "    else:\n",
    "        halfrange = np.abs(var - center).max(skipna=True)\n",
    "        normalizer = CenteredNorm(vcenter=center, halfrange=halfrange)\n",
    "\n",
    "    return normalizer(var.data)\n",
    "\n",
    "\n",
    "def exploire_layer(\n",
    "    arr,\n",
    "    cell_dim=\"cells\",\n",
    "    cmap=\"viridis\",\n",
    "    center=None,\n",
    "    alpha=None,\n",
    "):\n",
    "    from lonboard import SolidPolygonLayer\n",
    "    from lonboard.colormap import apply_continuous_cmap\n",
    "    from matplotlib import colormaps\n",
    "\n",
    "    if len(arr.dims) != 1 or cell_dim not in arr.dims:\n",
    "        raise ValueError(\n",
    "            f\"exploration only works with a single dimension ('{cell_dim}')\"\n",
    "        )\n",
    "\n",
    "    cell_ids = arr.dggs.coord.data\n",
    "    grid_info = arr.dggs.grid_info\n",
    "\n",
    "    polygons = grid_info.cell_boundaries(cell_ids, backend=\"geoarrow\")\n",
    "\n",
    "    normalized_data = normalize(arr.variable, center=center)\n",
    "\n",
    "    colormap = colormaps[cmap]\n",
    "    colors = apply_continuous_cmap(normalized_data, colormap, alpha=alpha)\n",
    "\n",
    "    table = create_arrow_table(polygons, arr)\n",
    "    layer = SolidPolygonLayer(table=table, filled=True, get_fill_color=colors)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lonboard\n",
    "import numpy as np\n",
    "\n",
    "#use of tanh to concentrate the scale variation for the lower values\n",
    "ds1=ds.b02.compute()\n",
    "\n",
    "lonboard.Map(\n",
    "    [\n",
    "        exploire_layer(\n",
    "            ds1,\n",
    "            alpha=0.33,\n",
    "            cmap='Reds'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import h3\n",
    "from pyproj import Transformer\n",
    "\n",
    "# 1. Load Zarr Data\n",
    "zarr_path = \"/home/ubuntu/project/eopf-safe-2-healpix/src/notebook/S2A_MSIL2A_20240511T001111_N0510_R073_T56KKB_20240511T025552.zarr\"\n",
    "dt = xr.open_datatree(zarr_path, engine=\"zarr\", mask_and_scale=False, chunks={})\n",
    "ds = dt.measurements.reflectance.r60m\n",
    "\n",
    "# 2. Prepare grid (x/y → lat/lon)\n",
    "x = ds['x'].values\n",
    "y = ds['y'].values\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "utm_crs = \"EPSG:32632\"\n",
    "transformer = Transformer.from_crs(utm_crs, \"EPSG:4326\", always_xy=True)\n",
    "lon, lat = transformer.transform(xx, yy)\n",
    "lat = lat.ravel()\n",
    "lon = lon.ravel()\n",
    "\n",
    "# 3. H3 mapping (only once)\n",
    "resolution = 9\n",
    "print(\"Computing H3 cell IDs...\")\n",
    "h3_cells = [h3.latlng_to_cell(lat[i], lon[i], resolution) for i in tqdm(range(len(lat)))]\n",
    "h3_ids = [h3.str_to_int(c) for c in h3_cells]  # uint64\n",
    "\n",
    "# 4. Aggregate each band\n",
    "bands = ['b02', 'b03', 'b04']\n",
    "data_vars = {}\n",
    "\n",
    "for band in bands:\n",
    "    print(f\"Processing band: {band}\")\n",
    "    vals = ds[band].values.ravel()\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'cell_id': np.array(h3_ids),\n",
    "        'value': vals\n",
    "    })\n",
    "\n",
    "    agg = df.groupby('cell_id').mean().reset_index()\n",
    "\n",
    "    da = xr.DataArray(\n",
    "        data=agg[\"value\"].values.astype(np.float64),\n",
    "        dims=[\"cells\"],\n",
    "        coords={\"cell_ids\": (\"cells\", agg[\"cell_id\"].values)},\n",
    "        attrs={\n",
    "            \"grid_name\": \"h3\",\n",
    "            \"level\": resolution,\n",
    "            \"units\": \"reflectance\",\n",
    "            \"long_name\": f\"BOA Reflectance {band.upper()}\"\n",
    "        }\n",
    "    )\n",
    "    data_vars[band] = da\n",
    "\n",
    "# 5. Build dataset\n",
    "h3_dataset = xr.Dataset(\n",
    "    data_vars=data_vars,\n",
    "    attrs={\n",
    "        \"Conventions\": \"COARDS\",\n",
    "        \"title\": \"Sentinel-2 Bands Reflectance (H3 Aggregated)\",\n",
    "        \"description\": (\n",
    "            \"BOA reflectance from MSI acquisition at spectral bands 02 (490 nm), \"\n",
    "            \"03 (560 nm), and 04 (665 nm), aggregated into H3 cells at resolution \"\n",
    "            f\"{resolution}.\"\n",
    "        ),\n",
    "        \"grid_name\": \"h3\",\n",
    "        \"level\": resolution\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add grid info to coordinate\n",
    "h3_dataset.coords[\"cell_ids\"].attrs[\"grid_name\"] = \"h3\"\n",
    "h3_dataset.coords[\"cell_ids\"].attrs[\"level\"] = resolution\n",
    "\n",
    "# 6. Save to Zarr\n",
    "h3_dataset.to_zarr(\"sentinel_rgb_h3_dataset.zarr\", mode=\"w\")\n",
    "print(\"Saved to sentinel_rgb_h3_dataset.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset (if not already in memory)\n",
    "original_ds = xr.open_zarr(\"/home/ubuntu/project/eopf-safe-2-healpix/src/notebook/sentinel_rgb_h3_dataset.zarr\")\n",
    "ds = original_ds.pipe(xdggs.decode)\n",
    "cell_centers = ds.dggs.cell_centers()\n",
    "cell_boundaries = ds.dggs.cell_boundaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lonboard\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "lonboard.Map(\n",
    "    [\n",
    "        exploire_layer(\n",
    "            ds.b04.compute(),\n",
    "            alpha=1,\n",
    "            cmap='Reds'\n",
    "        ),\n",
    "        exploire_layer(\n",
    "            ds.b03.compute(),\n",
    "            alpha=1,\n",
    "            cmap='Greens'),\n",
    "        exploire_layer(\n",
    "            ds.b02.compute(),\n",
    "            alpha=1,\n",
    "            cmap='Blues'),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eopf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
